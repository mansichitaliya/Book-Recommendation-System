# -*- coding: utf-8 -*-
"""Testing with Comparison.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18qupupfZcGyoYtQaB8jsaECFWOYZ-Ig2
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install surprise

import pandas as pd
from surprise import Reader
from surprise import Dataset
from surprise.model_selection import cross_validate
from surprise import NormalPredictor
from surprise import KNNBasic
from surprise import KNNWithMeans
from surprise import KNNWithZScore
from surprise import KNNBaseline
from surprise import SVD
from surprise import BaselineOnly
from surprise import SVDpp
from surprise import NMF
from surprise import SlopeOne
from surprise import CoClustering
from surprise.accuracy import rmse
from surprise import accuracy
from surprise.model_selection import train_test_split

copied_path = '/Users/prithvi/Desktop/Develop/Book Recommendation System/BX-Users.csv' #remove ‘content/’ from path then use 
user = pd.read_csv(copied_path, sep=';', error_bad_lines=False, encoding="latin-1")
user.columns = ['userID', 'Location', 'Age']

copied_path = '/Users/prithvi/Desktop/Develop/Book Recommendation System/BX-Book-Ratings.csv' #remove ‘content/’ from path then use
rating = pd.read_csv(copied_path, sep=';', error_bad_lines=False, encoding="latin-1")
rating.columns = ['userID', 'ISBN', 'bookRating']

user.head()

rating.head()

df = pd.merge(user, rating, on='userID', how='inner')
df.drop(['Location', 'Age'], axis=1, inplace=True)

df.head()

df.shape

df.info()

print('Dataset shape: {}'.format(df.shape))
print('-Dataset examples-')
print(df.iloc[::200000, :])

!pip install chart_studio

"""Most of the users gave less than 5 ratings, and very few users gave many ratings, although the most productive user have given 13,602 ratings.

I'm sure you have noticed that the above two charts share the same distribution. The number of ratings per movie and the number of ratings per user decay exponentially.

To reduce the dimensionality of the dataset, we will filter out rarely rated movies and rarely rating users.
"""

filter_books = df['ISBN'].value_counts() > min_book_ratings
filter_books = filter_books[filter_books].index.tolist()

min_user_ratings = 50
filter_users = df['userID'].value_counts() > min_user_ratings
filter_users = filter_users[filter_users].index.tolist()

df_new = df[(df['ISBN'].isin(filter_books)) & (df['userID'].isin(filter_users))]
print('The original data frame shape:\t{}'.format(df.shape))
print('The new data frame shape:\t{}'.format(df_new.shape))

"""**Surprise**

To load a dataset from a pandas dataframe, we will use the load_from_df() method, we will also need a Reader object, and the rating_scale parameter must be specified. The dataframe must have three columns, corresponding to the user ids, the item ids, and the ratings in this order. Each row thus corresponds to a given rating.
"""

reader = Reader(rating_scale=(0, 9))
data = Dataset.load_from_df(df_new[['userID', 'ISBN', 'bookRating']], reader)

"""With the Surprise library, we will benchmark the following algorithms

**Basic algorithms**

**NormalPredictor**

NormalPredictor algorithm predicts a random rating based on the distribution of the training set, which is assumed to be normal. This is one of the most basic algorithms that do not do much work.

**k-NN algorithms**

**KNNBasic**

KNNBasic is a basic collaborative filtering algorithm.

**KNNWithMeans**

KNNWithMeans is basic collaborative filtering algorithm, taking into account the mean ratings of each user.

**KNNWithZScore**

KNNWithZScore is a basic collaborative filtering algorithm, taking into account the z-score normalization of each user.

**KNNBaseline**

KNNBaseline is a basic collaborative filtering algorithm taking into account a baseline rating.

**Matrix Factorization-based algorithms**

**SVD**

SVD algorithm is equivalent to Probabilistic Matrix Factorization (http://papers.nips.cc/paper/3208-probabilistic-matrix-factorization.pdf)

**SVDpp**

The SVDpp algorithm is an extension of SVD that takes into account implicit ratings.


We use rmse as our accuracy metric for the predictions.
"""

benchmark = []
# Iterate over all algorithms
for algorithm in [SVD(), NormalPredictor(), KNNBaseline(), KNNBasic(), KNNWithMeans(), KNNWithZScore(), SVDpp()]:
    # Perform cross validation
    results = cross_validate(algorithm, data, measures=['RMSE'], cv=3, verbose=False)
    
    # Get results & append algorithm name
    tmp = pd.DataFrame.from_dict(results).mean(axis=0)
    tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))
    benchmark.append(tmp)

surprise_results = pd.DataFrame(benchmark).set_index('Algorithm').sort_values('test_rmse')

surprise_results

algo = KNNWithMeans()
cross_validate(algo, data, measures=['RMSE'], cv=3, verbose=False)

trainset, testset = train_test_split(data, test_size=0.25)
algo = KNNWithMeans()
predictions = algo.fit(trainset).test(testset)
accuracy.rmse(predictions)

trainset = algo.trainset
print(algo.__class__.__name__)