# -*- coding: utf-8 -*-
"""Collaborative and Content Based Combined.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P8OQ9fPvzqNchsCDr2sWG5lnllWUpR_8
"""

import pandas as pd
import matplotlib.pyplot as plt
import sklearn.metrics as metrics
import numpy as np
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import correlation
from sklearn.metrics.pairwise import pairwise_distances
import ipywidgets as widgets
from IPython.display import display, clear_output
from contextlib import contextmanager
import warnings
warnings.filterwarnings('ignore')
import numpy as np
import os, sys
import re
import seaborn as sns

from IPython.display import HTML

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import RegexpTokenizer
import re
import string
import random
from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt
# %matplotlib inline

#copied_path = 'drive/MyDrive/datasets/BX-Books.csv' #remove ‘content/’ from path then use 
#data = pd.read_csv(copied_path)
books = pd.read_csv("BX-Books.csv", sep=';', on_bad_lines = "skip", encoding="latin-1")
books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']


#copied_path = 'drive/MyDrive/datasets/BX-Users.csv' #remove ‘content/’ from path then use 
users = pd.read_csv("BX-Users.csv", sep=';', on_bad_lines = "skip", encoding="latin-1")
users.columns = ['userID', 'Location', 'Age']

#copied_path = 'drive/MyDrive/datasets/BX-Book-Ratings.csv' #remove ‘content/’ from path then use
ratings = pd.read_csv("BX-Book-Ratings.csv", sep=';', on_bad_lines = "skip", encoding="latin-1")
ratings.columns = ['userID', 'ISBN', 'bookRating']


#dropping last three columns containing image URLs which will not be required for analysis
#books.drop(['imageUrlS', 'imageUrlM', 'imageUrlL'],axis=1,inplace=True)

#making this setting to display full text in columns
pd.set_option('display.max_colwidth', +1)

#making required corrections as above, keeping other fields intact
books.loc[books.ISBN == '2070426769','yearOfPublication'] = 2003
books.loc[books.ISBN == '2070426769','bookAuthor'] = "Jean-Marie Gustave Le ClÃ?Â©zio"
books.loc[books.ISBN == '2070426769','publisher'] = "Gallimard"
books.loc[books.ISBN == '2070426769','bookTitle'] = "Peuple du ciel, suivi de 'Les Bergers"

#Correcting the dtypes of yearOfPublication
books.yearOfPublication=pd.to_numeric(books.yearOfPublication, errors='coerce')

#However, the value 0 is invalid and as this dataset was published in 2004, I have assumed the the years after 2006 to be 
#invalid keeping some margin in case dataset was updated thereafer
#setting invalid years as NaN
books.loc[(books.yearOfPublication > 2006) | (books.yearOfPublication == 0),'yearOfPublication'] = np.NAN

#replacing NaNs with mean value of yearOfPublication
books.yearOfPublication.fillna(round(books.yearOfPublication.mean()), inplace=True)

#resetting the dtype as int32
books.yearOfPublication = books.yearOfPublication.astype(np.int32)

#since there is nothing in common to infer publisher for NaNs, replacing these with 'other
books.loc[(books.ISBN == '193169656X'),'publisher'] = 'other'
books.loc[(books.ISBN == '1931696993'),'publisher'] = 'other'

#In my view values below 5 and above 90 do not make much sense for our book rating case...hence replacing these by NaNs
users.loc[(users.Age > 90) | (users.Age < 5), 'Age'] = np.nan

#replacing NaNs with mean
users.Age = users.Age.fillna(users.Age.mean())

#setting the data type as int
users.Age = users.Age.astype(np.int32)

#ratings dataset should have books only which exist in our books dataset, unless new books are added to books dataset
ratings_new = ratings[ratings.ISBN.isin(books.ISBN)]

#ratings dataset should have ratings from users which exist in users dataset, unless new users are added to users dataset
ratings = ratings[ratings.userID.isin(users.userID)]

#ratings dataset will have n_users*n_books entries if every user rated every item, this shows that the dataset is very sparse
n_users = users.shape[0]
n_books = books.shape[0]
#Sparsity of dataset in %
sparsity=1.0-len(ratings_new)/float(n_users*n_books)

#Hence segragating implicit and explict ratings datasets
ratings_explicit = ratings_new[ratings_new.bookRating != 0]
ratings_implicit = ratings_new[ratings_new.bookRating == 0]


from sklearn.metrics.pairwise import cosine_similarity

"""**Simple Rating Based Recommendation System**"""

#At this point , a simple popularity based recommendation system can be built based on count of user ratings for different books
def RatingBasedRecommendation():
    ratings_count = pd.DataFrame(ratings_explicit.groupby(['ISBN'])['bookRating'].sum())
    #ratings_count_avg = pd.DataFrame(ratings_explicit.groupby(['ISBN'])['bookRating'].mean())
    top10 = ratings_count.sort_values('bookRating', ascending = False).head(10)
    Top10=top10.merge(books, left_index = True, right_on = 'ISBN')
    data=[]
    for i in range(10):
        res={}
        res["name"]=books.bookTitle[Top10.index[i]]
        res["url"]=books.imageUrlM[Top10.index[i]]
        res["author"]=books.bookAuthor[Top10.index[i]]
        res["year"]=books.yearOfPublication[Top10.index[i]]
        res["publisher"]=books.publisher[Top10.index[i]]
        data.append(res)
    #i=books.imageUrlL[Top10.index[i]]
    #response = requests.get(i)
    #img = Image.open(BytesIO(response.content))
    #plt.figure()
    #plt.imshow(img)
    return data 
RatingBasedRecommendation() 
#Given below are top 10 recommendations based on popularity. It is evident that books authored by J.K. Rowling are most popular
