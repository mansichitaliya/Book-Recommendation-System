# -*- coding: utf-8 -*-
"""Content Based Filtering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dc9HNjKnmXFC0pPUt4EMKl1WV7p3wlxG
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import RegexpTokenizer
import re
import string
import random
from PIL import Image
import requests
from io import BytesIO
import matplotlib.pyplot as plt
# %matplotlib inline

# Reading the file
copied_path = 'data.csv' #remove ‘content/’ from path then use 
df = pd.read_csv(copied_path)

#Reading the first five records
#df.head()

#Checking the shape of the file
#df.shape

# Genre distribution
df['genre'].value_counts().plot(x = 'genre', y ='count', kind = 'bar', figsize = (10,5)  )

# Printing the book title and description randomly
#print(df['title'] [2300])
#print()
#print(df['Desc'][2300])
#
# Printing the book title and description randomly
#print(df['title'] [367])
#print(df['Desc'][367])

# Calculating the word count for book description
df['word_count'] = df['Desc'].apply(lambda x: len(str(x).split()))# Plotting the word count
df['word_count'].plot(
    kind='hist',
    bins = 50,
    figsize = (12,8),title='Word Count Distribution for book descriptions')

import nltk
#nltk.download('punkt')
#nltk.download('averaged_perceptron_tagger')
#nltk.download('stopwords')

from textblob import TextBlob
blob = TextBlob(str(df['Desc']))
pos_df = pd.DataFrame(blob.tags, columns = ['word' , 'pos'])
pos_df = pos_df.pos.value_counts()[:20]
pos_df.plot(kind = 'bar', figsize=(10, 8), title = "Top 20 Part-of-speech tagging for comments")

#Converting text descriptions into vectors using TF-IDF using Bigram
tf = TfidfVectorizer(ngram_range=(2, 2), stop_words='english', lowercase = False)
tfidf_matrix = tf.fit_transform(df['Desc'])
total_words = tfidf_matrix.sum(axis=0) 
#Finding the word frequency
freq = [(word, total_words[0, idx]) for word, idx in tf.vocabulary_.items()]
freq =sorted(freq, key = lambda x: x[1], reverse=True)
#converting into dataframe 
bigram = pd.DataFrame(freq)
bigram.rename(columns = {0:'bigram', 1: 'count'}, inplace = True) 
#Taking first 20 records
bigram = bigram.head(20)
#Plotting the bigram distribution
bigram.plot(x ='bigram', y='count', kind = 'bar', title = "Bigram disribution for the top 20 words in the book description", figsize = (15,7), )

#Converting text descriptions into vectors using TF-IDF using Trigram
tf = TfidfVectorizer(ngram_range=(3, 3), stop_words='english', lowercase = False)
tfidf_matrix = tf.fit_transform(df['Desc'])
total_words = tfidf_matrix.sum(axis=0) 
#Finding the word frequency
freq = [(word, total_words[0, idx]) for word, idx in tf.vocabulary_.items()]
freq =sorted(freq, key = lambda x: x[1], reverse=True)
#converting into dataframe 
trigram = pd.DataFrame(freq)
trigram.rename(columns = {0:'trigram', 1: 'count'}, inplace = True) 
#Taking first 20 records
trigram = trigram.head(20)
#Plotting the trigramn distribution
trigram.plot(x ='trigram', y='count', kind = 'bar', title = "Bigram disribution for the top 20 words in the book description", figsize = (15,7), )

# Function for removing NonAscii characters
def _removeNonAscii(s):
    return "".join(i for i in s if  ord(i)<128)
# Function for converting into lower case
def make_lower_case(text):
    return text.lower()
# Function for removing stop words
def remove_stop_words(text):
    text = text.split()
    stops = set(stopwords.words("english"))
    text = [w for w in text if not w in stops]
    text = " ".join(text)
    return text
# Function for removing punctuation
def remove_punctuation(text):
    tokenizer = RegexpTokenizer(r'\w+')
    text = tokenizer.tokenize(text)
    text = " ".join(text)
    return text
#Function for removing the html tags
def remove_html(text):
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)
# Applying all the functions in description and storing as a cleaned_desc
df['cleaned_desc'] = df['Desc'].apply(_removeNonAscii)
df['cleaned_desc'] = df.cleaned_desc.apply(func = make_lower_case)
df['cleaned_desc'] = df.cleaned_desc.apply(func = remove_stop_words)
df['cleaned_desc'] = df.cleaned_desc.apply(func=remove_punctuation)
df['cleaned_desc'] = df.cleaned_desc.apply(func=remove_html)

from sklearn.metrics.pairwise import cosine_similarity

def checkavailable(m_name):
    df2 = df.reset_index()
    indices = pd.Series(df2.index, index=df2['title'])
    all_titles = [df2['title'][i] for i in range(len(df['title']))]
    #print(all_titles)
    #all_titles=all_titles.split(',')
    # name=str(m_name)
    # print(name)
    # name.lower()
    # name=m_name.split(' ')
    # name=' '.join(map(str, name))
    # for i in range(len(all_titles)):
    #     z=all_titles[i]
    #     z.lower()
        
    #     z=z.split(' ')
    #     z=' '.join(map(str, z))        
    #     print(name, z)
    #     if(name==z):
    #         print('this works')
    #         break
    #all_titles.index(m_name)
    #all_titles=all_titles.split(',')
    # for i in range(15):
    #     #print(all_titles[i], m_name)
    # # for i in all_titles:
    # #     if(i==m_name):
    # #         #print('AAAAAAAAAAAAAA')
    # #         break
    # if any(m_name in s for s in all_titles):
    #     #print("TRUE")
    #     return(True)
    if m_name not in all_titles:
        #print('F')
        return (False)
    else:
        #print('T')
        return (True)

# checkavailable('The Eyre Affair')
# checkavailable('The Green Mile')
# The Snow Child
# The Help
# The Partner
# The Road Ahead
# Steve Jobs
# REBECCA
# Hollow City
# THE SILVER CHAIR
# The Da Vinci Code
# Angels & Demons
# Steppenwolf
# The Raven Boys
# WE WERE LIARS
# PRACTICAL MAGIC
# THE HUNGER GAMES
# CATCHING FIRE
# THE LOST SYMBOL
# ANANSI BOYS
# THE FINAL EMPIRE
# OTHELLO
# CAROL
# Rework
# Brooklyn
# The Notebook
# Brideshead Revisited
# A SEPARATE PEACE
# THE ALICE NETWORK
# CODE NAME VERITY
# ARTEMIS
# CASINO ROYALE
# THE FORGOTTEN GARDEN
# PRELUDES & NOCTURNES
# ALLEGIANT
# insurgent
# CODE NAME VERITY
# The Road Ahead
# The Infinite Game
# ANCILLARY JUSTICE
# NEUROMANCER
# BRITT-MARIE WAS HERE
# KINDRED
# THE LINCOLN LAWYER
# The Bean Trees
# The Giving Tree
# WATERSHIP DOWN
#checkavailable("Steve Jobs2")
def findgenre(m_name):
    answer=df.loc[df['title'] == m_name]
    return(answer['title'].to_string(index = False), answer['genre'].to_string(index = False))
    #print(answer.to_string(index = False))
#findgenre("Steve Jobs")
"""**Recommendation based on book title**"""
def allgenres():
    genre=[]
    for i in range(len(df['genre'])):
        if(df['genre'][i] not in genre):
            genre.append(df['genre'][i])
    print(genre)
#allgenres()
#['Business', 'Non-Fiction']
def allauthors():
    allauthors=[]
    for i in range(len(df['author'])):
        if(df['author'][i] not in allauthors):
            allauthors.append(df['author'][i])
    return(allauthors)



def topingenre(genre):
    genredata = df.loc[df['genre'] == genre]  
    genredata.reset_index(level = 0, inplace = True)
    genredata.sort_values("rating", ascending = False)
    returndata=[]
    name=[]
    url=[]
    author=[]
    rating=[]
    genre=[]
    Desc=[]
    for i in range(10):
        name.append(genredata['title'][i])
    for i in range(10):
        url.append(genredata['image_link'][i])
    for i in range(10):
        author.append(genredata['author'][i])
    for i in range(10):
        rating.append(genredata['rating'][i])
    for i in range(10):
        genre.append(genredata['genre'][i])
    for i in range(10):
        Desc.append(genredata['Desc'][i]) 
    #print(name, url, author, rating, genre, sep='\n')
    #print(rating, genre)
    #return (rating)    
    return (name, url, rating) 

def topratedcontent():
    topratedcontent = df
    #topratedcontent.reset_index(level = 0, inplace = True)
    topratedcontent.sort_values("rating", ascending = False)
    returndata=[]
    name=[]
    url=[]
    author=[]
    rating=[]
    genre=[]
    Desc=[]
    for i in range(10):
        name.append(topratedcontent['title'][i])
    for i in range(10):
        url.append(topratedcontent['image_link'][i])
    for i in range(10):
        author.append(topratedcontent['author'][i])
    for i in range(10):
        rating.append(topratedcontent['rating'][i])
    for i in range(10):
        genre.append(topratedcontent['genre'][i])
    for i in range(10):
        Desc.append(topratedcontent['Desc'][i]) 
    #print(name, url, author, rating, genre, sep='\n')
    #print(rating, genre)
    #return (rating)
    return (name, url, rating)     
    # return (name, url, author, rating, genre, Desc) 
#topratedcontent()
#topingenre('Business')
#topingenre('Non-Fiction')
def randombooks():
    topratedcontent = df.sample(n = 5)
    topratedcontent.reset_index(level = 0, inplace = True)
    #print(topratedcontent)
    #topratedcontent.sort_values("rating", ascending = False)
    returndata=[]
    name=[]
    url=[]
    author=[]
    rating=[]
    genre=[]
    Desc=[]
    for i in range(5):
        name.append(topratedcontent['title'][i])
    for i in range(5):
        url.append(topratedcontent['image_link'][i])
    for i in range(5):
        author.append(topratedcontent['author'][i])
    for i in range(5):
        rating.append(topratedcontent['rating'][i])
    for i in range(5):
        genre.append(topratedcontent['genre'][i])
    for i in range(5):
        Desc.append(topratedcontent['Desc'][i]) 
    #print(name, url, author, rating, genre, sep='\n')
    #print(rating, genre)
    #return (rating)
    return (name, url, rating)     
#randombooks()

# Function for recommending books based on Book title. It takes book title and genre as an input.
def recommendtitle(title, genre):
    
    # Matching the genre with the dataset and reset the index
    data = df.loc[df['genre'] == genre]  
    data.reset_index(level = 0, inplace = True) 
  
    # Convert the index into series
    indices = pd.Series(data.index, index = data['title'])
    
    #Converting the book title into vectors and used bigram
    tf = TfidfVectorizer(analyzer='word', ngram_range=(2, 2), min_df = 1, stop_words='english')
    tfidf_matrix = tf.fit_transform(data['title'])
    
    # Calculating the similarity measures based on Cosine Similarity
    sg = cosine_similarity(tfidf_matrix, tfidf_matrix)
    
    # Get the index corresponding to original_title
       
    idx = indices[title]
# Get the pairwsie similarity scores 
    sig = list(enumerate(sg[idx]))
# Sort the books
    sig = sorted(sig, key=lambda x: x[1], reverse=True)
# Scores of the 5 most similar books 
    sig = sig[1:6]
# Book indicies
    movie_indices = [i[0] for i in sig]
   
    # Top 5 book recommendation
    #rec = data[['title', 'image_link']].iloc[movie_indices]
       
    # It reads the top 5 recommend book url and print the images
    #for i in rec['title']:
    #    print(i)
    #for i in rec['image_link']:
    #    response = requests.get(i)
    #    img = Image.open(BytesIO(response.content))
    #    plt.figure()
    #    plt.imshow(img)
    returndata=[]
    rec = data.iloc[movie_indices]
    # for i in rec:
    # # It reads the top 5 recommend book url and print the images
    #     res={}
    #     res["name"]=i['title']
    #     res["url"]=i['image_link']
    #     res["author"]=i['author']
    #     res["rating"]=i['rating']
    #     res["desc"]=i['Desc']
    #     res["genre"]=i['genre']
    #     print(res["name"], res["url"], res["author"], res["rating"],  res["genre"])
    #     print('ENNNNNNNNDDDDDDDDDDDD')
    #     returndata.append(res)
    
    name=[]
    url=[]
    author=[]
    rating=[]
    genre=[]
    Desc=[]
    for i in rec['title']:
        name.append(i)
    for i in rec['image_link']:
        url.append(i)
    for i in rec['author']:
        author.append(i)
    for i in rec['rating']:
        rating.append(i)
    for i in rec['genre']:
        genre.append(i)
    for i in rec['Desc']:
        Desc.append(i) 
    # for i in range(len(name)):
    #     res={}
    #     res["name"]=str(name[i])
    #     res["url"]=str(url[i])
    #     res["author"]=str(author[i])
    #     res["rating"]=str(rating[i])
    #     res["Desc"]=str(Desc[i])
    #     res["genre"]=str(genre[i])
    #     print(res["name"], res["url"], res["author"], res["rating"],  res["genre"])
    #     #print(res)
    #     returndata.append(rec)
    #print(returndata)
    #i=books.imageUrlL[Top10.index[i]]
    #response = requests.get(i)
    #img = Image.open(BytesIO(response.content))
    #plt.figure()
    #plt.imshow(img)
    return (name, url, author, rating, genre, Desc) 

#recommendtitle("Steve Jobs", "Business")

"""**Recommendation based on book description**"""

# Function for recommending books based on Book title. It takes book title and genre as an input.
def recommenddesc(title, genre):
    
    global rec
    # Matching the genre with the dataset and reset the index
    data = df.loc[df['genre'] == genre]  
    data.reset_index(level = 0, inplace = True) 
  
    # Convert the index into series
    indices = pd.Series(data.index, index = data['title'])
    
    #Converting the book description into vectors and used bigram
    tf = TfidfVectorizer(analyzer='word', ngram_range=(2, 2), min_df = 1, stop_words='english')
    tfidf_matrix = tf.fit_transform(data['cleaned_desc'])
    
    # Calculating the similarity measures based on Cosine Similarity
    sg = cosine_similarity(tfidf_matrix, tfidf_matrix)
    
    # Get the index corresponding to original_title
       
    idx = indices[title]
# Get the pairwsie similarity scores 
    sig = list(enumerate(sg[idx]))
# Sort the books
    sig = sorted(sig, key=lambda x: x[1], reverse=True)
# Scores of the 5 most similar books 
    sig = sig[1:6]
# Book indicies
    movie_indices = [i[0] for i in sig]
    returndata=[]
    # Top 5 book recommendation
    #rec = data.iloc[movie_indices]
    returndata=[]
    rec = data.iloc[movie_indices]
    # for i in rec:
    # # It reads the top 5 recommend book url and print the images
    #     res={}
    #     res["name"]=i['title']
    #     res["url"]=i['image_link']
    #     res["author"]=i['author']
    #     res["rating"]=i['rating']
    #     res["desc"]=i['Desc']
    #     res["genre"]=i['genre']
    #     print(res["name"], res["url"], res["author"], res["rating"],  res["genre"])
    #     print('ENNNNNNNNDDDDDDDDDDDD')
    #     returndata.append(res)
    target=df.loc[df['title'] == title]  
    #df[data['col'] == value]
     
    name=[]
    url=[]
    author=[]
    rating=[]
    genre=[]
    Desc=[]
    name.append(target['title'].to_string(index = False))
    url.append(target['image_link'].to_string(index = False))
    author.append(target['author'].to_string(index = False))
    rating.append(target['rating'].to_string(index = False))
    genre.append(target['genre'].to_string(index = False))
    Desc.append(target['Desc'].to_string(index = False))
    print(name, url, author, rating, genre)
    for i in rec['title']:
        name.append(i)
    for i in rec['image_link']:
        url.append(i)
    for i in rec['author']:
        author.append(i)
    for i in rec['rating']:
        rating.append(i)
    for i in rec['genre']:
        genre.append(i)
    for i in rec['Desc']:
        Desc.append(i) 
    # for i in range(len(name)):
    #     res={}
    #     res["name"]=str(name[i])
    #     res["url"]=str(url[i])
    #     res["author"]=str(author[i])
    #     res["rating"]=str(rating[i])
    #     res["Desc"]=str(Desc[i])
    #     res["genre"]=str(genre[i])
    #     print(res["name"], res["url"], res["author"], res["rating"],  res["genre"])
    #     #print(res)
    #     returndata.append(rec)
    #print(returndata)
    #i=books.imageUrlL[Top10.index[i]]
    #response = requests.get(i)
    #img = Image.open(BytesIO(response.content))
    #plt.figure()
    #plt.imshow(img)
    return (name, url, author, rating, genre, Desc) 

    #for i in rec['title']:
    #    print(i)
    #for i in rec['Desc']:
    #    print(i)
    #for i in rec['rating']:
    #    print(i,"/5")
    #for i in rec['image_link']:
    #    response = requests.get(i)
    #    img = Image.open(BytesIO(response.content))
    #    plt.figure()
    #    plt.imshow(img)

#recommenddesc("Harry Potter and the Prisoner of Azkaban", "Non-Fiction")

#recommenddesc("Norwegian Wood", "Non-Fiction")